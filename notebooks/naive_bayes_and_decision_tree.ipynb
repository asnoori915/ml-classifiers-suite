{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "eCoQRW-5hqAy"
   },
   "outputs": [],
   "source": [
    "# cell 1: imports and data\n",
    "!mkdir -p datasets\n",
    "!wget -q -O datasets/mushroom.csv https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
    "!wget -q -O datasets/votes.csv https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "nOuMji5FiIuv"
   },
   "outputs": [],
   "source": [
    "# cell 2: encodes each column in the dataset and returns splits\n",
    "\n",
    "def splits(data):\n",
    "    for c in data.columns: # goes through every column\n",
    "        var = data[c].astype(str) # changes them to strings\n",
    "        data[c] = LabelEncoder().fit_transform(var) # encodes them\n",
    "\n",
    "    X = data.drop(columns=[0])  # all but first column\n",
    "    y = data[0]  # first column is label\n",
    "\n",
    "    # 80% for training, 20 is then split into 10 for dev and 10 for test\n",
    "    D_80, D_20, Y_80, Y_20 = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "    D_dev, D_test, Y_dev, Y_test = train_test_split(D_20, Y_20, test_size=0.5, stratify=Y_20, random_state=1)\n",
    "    return D_80, D_dev, D_test, Y_80, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "aA4voNSsiI7h"
   },
   "outputs": [],
   "source": [
    "# cell 3: load mushroom and vote datasets and prep splits\n",
    "\n",
    "mushroom = pd.read_csv(\"datasets/mushroom.csv\", header=None).fillna(\"missing\")\n",
    "votes = pd.read_csv(\"datasets/votes.csv\", header=None).replace(\"?\", \"missing\")\n",
    "\n",
    "# performs splits function on training, test, dev sets\n",
    "mushroomTrain, mushroomDev, mushroomTest, mushroomY_Train, mushroomY_Dev, mushroomY_Test = splits(mushroom)\n",
    "votesTrain, votesDev, votesTest, votesY_Train, votesY_Dev, votesY_Test = splits(votes)\n",
    "\n",
    "# testing purposes\n",
    "# print(\"mushroom split:\", mushroomTrain.shape, mushroomDev.shape, mushroomTest.shape)\n",
    "# print(\"vote split:\", votesTrain.shape, votesDev.shape, votesTest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "1ZrYvIEfiJJT"
   },
   "outputs": [],
   "source": [
    "# cell 4: Naive Bayes --> influenced by lecture pseudocode (lec 10)\n",
    "\n",
    "# estimate P(y) for each class y using laplace smoothing\n",
    "def prior_prob(Y, alpha=1.0):\n",
    "    #labels and count each class\n",
    "    cY = list(Counter(Y).keys())\n",
    "    cC = list(Counter(Y).values())\n",
    "\n",
    "    laplace = len(Y) + alpha * len(cY)\n",
    "    res = dict(zip(cY, [(num + alpha)/ laplace for num in cC]))\n",
    "    return res, cY\n",
    "\n",
    "# estimate P(x_j | y) for each feature j and class y using laplace smoothing\n",
    "def like(D, Y, labels, alpha=1.0):\n",
    "    n, d = D.shape\n",
    "    chance = {}\n",
    "    for label in labels:\n",
    "        rows = D[Y == label]  # get rows with the label\n",
    "        chance[label] = []\n",
    "        for j in range(d):\n",
    "            column = rows[:, j]  # get column j for label\n",
    "            num = Counter(column)  # count each value in column\n",
    "            origin = set(D[:, j])  # all values in the original feature\n",
    "\n",
    "            laplace = len(column) + alpha * len(origin)\n",
    "            # use laplace\n",
    "            sol = dict(zip(origin, [(num[v] + alpha) / laplace for v in origin]))\n",
    "            chance[label].append(sol)\n",
    "    return chance\n",
    "\n",
    "# Step 3: Prediction function\n",
    "def NB(D, res, like, labels):\n",
    "    D = np.array(D)\n",
    "    result = []\n",
    "\n",
    "    for row in D:  # for each example in the dataset\n",
    "        num = {}\n",
    "        for label in labels:  # try each label\n",
    "            score = np.log(res[label])  # start with log of prior probability\n",
    "            for j in range(len(row)):  # check each feature\n",
    "                if row[j] in like[label][j]:  # if value was seen in training\n",
    "                    chance = like[label][j][row[j]]\n",
    "                    score += np.log(chance)  #  add log likelihood\n",
    "            num[label] = score  #save total score for this label\n",
    "        result.append(max(num, key=num.get))  # add best label\n",
    "\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "xRySCi9PjhUY"
   },
   "outputs": [],
   "source": [
    "# cell 5: Decision Tree --> influenced by Decision Tree slides (lecs 12/13)\n",
    "\n",
    "# entropy of a label distribution\n",
    "def entropy(y):\n",
    "    e = 0\n",
    "    for i in Counter(y).values():\n",
    "        p = i / len(y)\n",
    "        if p > 0:\n",
    "            e -= p * np.log2(p)\n",
    "    return e\n",
    "\n",
    "# information gain for splitting on a feature\n",
    "def info_gain(y, column):\n",
    "    split = sum((len(y[column == v]) / len(y)) * entropy(y[column == v]) for v in set(column))\n",
    "    return entropy(y) - split\n",
    "\n",
    "# recursive function to build decision tree (slide 19 lec 13)\n",
    "def induce_tree(D, Y, depth, max_depth):\n",
    "    # if all labels are the same, return that label\n",
    "    if len(set(Y)) == 1 or depth == max_depth:\n",
    "        return Counter(Y).most_common(1)[0][0]\n",
    "\n",
    "    # get the best feature (f*) to split on\n",
    "    gain = [info_gain(Y, D[:, f]) for f in range(D.shape[1])]\n",
    "    f_star = np.argmax(gain)\n",
    "\n",
    "    # build children for each value k in Values(f*)\n",
    "    tree = (f_star, {})\n",
    "    for k in np.unique(D[:, f_star]):\n",
    "        x = D[:, f_star] == k\n",
    "        D_k, Y_k = D[x], Y[x]\n",
    "        tree[1][k] = induce_tree(D_k, Y_k, depth + 1, max_depth)\n",
    "    return tree\n",
    "\n",
    "def get_tree(D, tree, labels):\n",
    "    results = []\n",
    "    for row in D:  # go through each input row\n",
    "        node = tree\n",
    "        while isinstance(node, tuple):\n",
    "            column, path = node  # split on f\n",
    "            node = path.get(row[column], labels[0])  # go to branch or label\n",
    "        results.append(node)  # add\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUGlaCGIjm3y",
    "outputId": "9b4a8f2c-c21b-48ff-bc60-946bfa3f830f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes --> Mushroom Accuracy: 95.82 %\n",
      "[[419   2]\n",
      " [ 32 360]]\n",
      "Naive Bayes --> Vote Accuracy: 88.64 %\n",
      "[[23  4]\n",
      " [ 1 16]]\n",
      "\n",
      "\n",
      "Depth = 2 Accuracy = 99.26 %\n",
      "Depth = 3 Accuracy = 99.63 %\n",
      "Depth = 4 Accuracy = 100.0 %\n",
      "Depth = 5 Accuracy = 100.0 %\n",
      "Best depth: 4\n",
      "\n",
      "\n",
      "Decision Tree --> Mushroom Accuracy: 100.0 %\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Decision Tree --> Vote Accuracy: 95.45 %\n",
      "[[26  1]\n",
      " [ 1 16]]\n"
     ]
    }
   ],
   "source": [
    "# cell 6: train models and results\n",
    "\n",
    "# naive bayes training/predicting\n",
    "p_mushroom, labels_mushroom = prior_prob(mushroomY_Train.values)\n",
    "l_mushroom = like(mushroomTrain.values, mushroomY_Train.values, labels_mushroom)\n",
    "res_nb_mushroom = NB(mushroomTest.values, p_mushroom, l_mushroom, labels_mushroom)\n",
    "p_votes, labels_votes = prior_prob(votesY_Train.values)\n",
    "l_votes = like(votesTrain.values, votesY_Train.values, labels_votes)\n",
    "res_nb_votes = NB(votesTest.values, p_votes, l_votes, labels_votes)\n",
    "\n",
    "print(\"Naive Bayes --> Mushroom Accuracy:\", round(accuracy_score(mushroomY_Test, res_nb_mushroom) * 100, 2), \"%\")\n",
    "print(confusion_matrix(mushroomY_Test, res_nb_mushroom))\n",
    "print(\"Naive Bayes --> Vote Accuracy:\", round(accuracy_score(votesY_Test, res_nb_votes) * 100, 2), \"%\")\n",
    "print(confusion_matrix(votesY_Test, res_nb_votes))\n",
    "print(\"\\n\")\n",
    "\n",
    "# decision Tree: find best depth ---\n",
    "d = 2\n",
    "best_depth = None\n",
    "highest_acc = 0\n",
    "while True:\n",
    "    t_mushroom = induce_tree(mushroomTrain.values, mushroomY_Train.values, 0, d)\n",
    "    res_dev = get_tree(mushroomDev.values, t_mushroom, labels_mushroom)\n",
    "    a = accuracy_score(mushroomY_Dev, res_dev)\n",
    "    print(\"Depth =\", d, \"Accuracy =\", round(a * 100, 2), \"%\")\n",
    "    if a > highest_acc:\n",
    "        highest_acc = a\n",
    "        best_depth = d\n",
    "        d += 1  # try next depth\n",
    "    else:\n",
    "        break  # stop if accuracy stops getting better\n",
    "\n",
    "print(\"Best depth:\", best_depth)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# decision trees trained on best depth ---\n",
    "t_mushroom = induce_tree(mushroomTrain.values, mushroomY_Train.values, 0, best_depth)\n",
    "t_votes = induce_tree(votesTrain.values, votesY_Train.values, 0, best_depth)\n",
    "res_dt_mushroom = get_tree(mushroomTest.values, t_mushroom, labels_mushroom)\n",
    "res_dt_votes = get_tree(votesTest.values, t_votes, labels_votes)\n",
    "\n",
    "print(\"Decision Tree --> Mushroom Accuracy:\", round(accuracy_score(mushroomY_Test, res_dt_mushroom) * 100, 2), \"%\")\n",
    "print(confusion_matrix(mushroomY_Test, res_dt_mushroom))\n",
    "print(\"Decision Tree --> Vote Accuracy:\", round(accuracy_score(votesY_Test, res_dt_votes) * 100, 2), \"%\")\n",
    "print(confusion_matrix(votesY_Test, res_dt_votes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rbuqnk4kBo2",
    "outputId": "4fbba2be-a7fd-4c27-e6ff-2fd969076949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive-Bayes (Mushroom) Accuracy: 95.82 %\n",
      "Sklearn Decision Tree (Mushroom) Accuracy: 97.79 %\n",
      "\n",
      "\n",
      "Sklearn Naive-Bayes (Voting) Accuracy: 88.64 %\n",
      "Sklearn Decision Tree (Voting) Accuracy: 95.45 %\n"
     ]
    }
   ],
   "source": [
    "# sklearn models for mushroom\n",
    "sk_nb_m = CategoricalNB().fit(mushroomTrain, mushroomY_Train)\n",
    "sk_dt_m = DecisionTreeClassifier(max_depth=best_depth).fit(mushroomTrain, mushroomY_Train)\n",
    "\n",
    "# sklearn models for Votes\n",
    "sk_nb_v = CategoricalNB().fit(votesTrain, votesY_Train)\n",
    "sk_dt_v = DecisionTreeClassifier(max_depth=best_depth).fit(votesTrain, votesY_Train)\n",
    "\n",
    "print(\"Sklearn Naive-Bayes (Mushroom) Accuracy:\", round(accuracy_score(mushroomY_Test, sk_nb_m.predict(mushroomTest)) * 100, 2), \"%\")\n",
    "print(\"Sklearn Decision Tree (Mushroom) Accuracy:\", round(accuracy_score(mushroomY_Test, sk_dt_m.predict(mushroomTest)) * 100, 2), \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Sklearn Naive-Bayes (Voting) Accuracy:\", round(accuracy_score(votesY_Test, sk_nb_v.predict(votesTest)) * 100, 2), \"%\")\n",
    "print(\"Sklearn Decision Tree (Voting) Accuracy:\", round(accuracy_score(votesY_Test, sk_dt_v.predict(votesTest)) * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "cPYlOdxampsy"
   },
   "outputs": [],
   "source": [
    "readme = \"\"\"\n",
    "Naive Bayes and Decision Trees\n",
    "By: Amir Noori\n",
    "\n",
    "Overview:\n",
    "This homework uses two classification models being Naive Bayes and Decision Trees to test their accuracies with two different datasets. The datasets being a mushroom dataset to classify edibility and a congressional voting record dataset to classify political party.\n",
    "\n",
    "Key Features:\n",
    "First, the Naive Bayes classifier is implemented using Laplace smoothing.\n",
    "Then the mushroom and vote datasets are loaded and split with the Naive Bayes model tested on both.\n",
    "The same process is repeated using a Decision Tree classifier built from scratch using entropy and information gain.\n",
    "Each custom model is then compared to their sklearn equivalents to see how they match up.\n",
    "This is done by training the data under the classification models and printing and tracking their results.\n",
    "\n",
    "Files:\n",
    "- `mushroom.csv`: Mushroom classification dataset (UCI)\n",
    "- `votes.csv`: Voting records dataset (UCI)\n",
    "\n",
    "To run this:\n",
    "1. Open the Colab notebook.\n",
    "2. Run each cell from top to bottom (or all at once using the Runtime --> Run All shortcut).\n",
    "3. All files are automatically downloaded and preprocessed during execution.\n",
    "4. View resulting model accuracy and confusion matrices for both tasks.\n",
    "\n",
    "\"\"\"\n",
    "with open(\"README.txt\", \"w\") as f:\n",
    "    f.write(readme)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
